# Supabase Integration Setup Guide

Supabase provides a real-time PostgreSQL database that allows event data scraped by any user to be instantly shared with all other users viewing the same city. This creates a collaborative, shared event database.

## Why Supabase?

1. **Real-time Sync**: Events scraped by one user immediately appear for all users
2. **Shared Database**: All events are stored centrally, reducing duplicate scraping
3. **Easy Setup**: Zero infrastructure - Supabase is a managed service
4. **Cost-effective**: Free tier supports up to 500k events
5. **Built on PostgreSQL**: Same familiar database as local setup

## Setup Steps

### 1. Create a Supabase Project

1. Go to https://supabase.com
2. Click "Start your project" or sign in
3. Create a new project:
   - Organization: Create one or use existing
   - Project name: `nocturne-events`
   - Database password: Store securely
   - Region: Choose closest to your users
4. Wait for project to be created (2-3 minutes)

### 2. Get Your Credentials

1. In Supabase dashboard, go to "Settings" → "API"
2. Copy:
   - **Project URL** → Set as `SUPABASE_URL`
   - **anon public key** → Set as `SUPABASE_KEY`

These go in your `.env`:
```
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

### 3. Create Database Tables

1. Go to Supabase → SQL Editor
2. Create new query and paste this SQL:

```sql
-- Events table for sharing scraped events across all users
CREATE TABLE events (
    id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
    title TEXT NOT NULL,
    link TEXT UNIQUE NOT NULL,
    date DATE NOT NULL,
    time TEXT,
    location TEXT,
    description TEXT,
    source TEXT,
    city_id TEXT NOT NULL,
    synced_at TIMESTAMP DEFAULT now(),
    last_scraped TIMESTAMP DEFAULT now(),
    created_at TIMESTAMP DEFAULT now()
);

-- Indexes for performance
CREATE INDEX idx_events_city ON events(city_id);
CREATE INDEX idx_events_city_date ON events(city_id, date);
CREATE INDEX idx_events_last_scraped ON events(last_scraped DESC);
CREATE INDEX idx_events_source ON events(source);

-- Subscriptions table
CREATE TABLE subscriptions (
    id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
    email TEXT NOT NULL,
    city_id TEXT NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT now(),
    unsubscribed_at TIMESTAMP,
    UNIQUE(email, city_id)
);

-- Subscriptions indexes
CREATE INDEX idx_subscriptions_email ON subscriptions(email);
CREATE INDEX idx_subscriptions_city ON subscriptions(city_id, is_active);
```

3. Click "Run" to execute

### 4. Configure Row Level Security (RLS)

For public read access (recommended for public event data):

1. Go to "Authentication" → "Policies"
2. For `events` table:
   - Create policy "Enable read access for all users"
   - Click "For SELECT" 
   - "Using" expression: `TRUE`
   - Click "Save"

For `subscriptions` table (email collection):
   - Create policy "Enable all access"
   - Click "For all" 
   - "Using" expression: `TRUE`
   - Click "Save"

### 5. Install Supabase Python Client

```bash
pip install supabase
```

### 6. Verify Connection

The backend will automatically sync events to Supabase when scraping completes. Check logs:

```bash
# Watch for successful sync messages
tail -f logs/app.log | grep -i supabase
```

You should see:
```
INFO: Supabase client initialized successfully
INFO: Supabase sync result: {'status': 'success', 'synced': 42, 'updated': 15}
```

## How It Works

### Event Flow

```
Scraper runs in City A
    ↓
Events saved to local PostgreSQL
    ↓
Events synced to Supabase (real-time)
    ↓
User in City A queries events
    ↓
Gets events from local DB
    ↓
User in City B loads City A events
    ↓
Gets events from local DB (which was synced from Supabase)
```

### Progressive Event Loading

During refresh:
1. Scraper is triggered for city
2. Frontend starts polling every 1 second
3. Scraper finds events → saves locally → syncs to Supabase
4. Frontend fetches updated events every second
5. New events appear live with smooth animations
6. User sees event count: "Found 12 new events from live scrape"

## Monitoring Supabase

### Check Event Sync Status

1. Go to Supabase → Table Editor
2. Click `events` table
3. See all synced events across all cities
4. Sort by `last_scraped` to see most recent

### View Database Metrics

1. Go to Supabase → Database → Dashboard
2. Monitor:
   - Query count
   - Database size
   - Connections

## Troubleshooting

### Supabase sync not working

**Symptom**: Scraping completes but events don't appear in Supabase

**Cause**: Usually missing environment variables

**Fix**:
1. Check `.env` has `SUPABASE_URL` and `SUPABASE_KEY`
2. Verify credentials are correct
3. Check backend logs: `tail -f logs/app.log | grep -i supabase`
4. Look for error messages like "Supabase not connected"

### "Supabase not installed" warning

**Fix**:
```bash
pip install supabase
```

### Events syncing very slowly

**Cause**: Supabase free tier rate limits

**Fix**: 
- Batch operations (reduce sync frequency)
- Upgrade to paid plan for higher limits
- Use local database as cache, sync less frequently

### Duplicate events appearing

**Cause**: Same event synced multiple times

**Fix**: Check `unique(email, city_id)` constraint is set on subscriptions, and `unique(link)` on events

## Supabase Pricing

- **Free Tier**: 
  - 500,000 rows
  - 2GB storage
  - Perfect for testing
  
- **Pro Tier**: 
  - $25/month
  - Unlimited rows
  - 100GB storage
  - Higher API rates

For a typical city scraper with 10,000 events per city and 40 cities, you'd use ~400k rows - fitting comfortably in free tier.

## Advanced: Custom Sync Logic

To customize event sync behavior, edit `backend/supabase_integration.py`:

```python
# Example: Only sync events with specific sources
async def sync_events(self, events_data: List[Dict], city_id: str) -> Dict:
    filtered = [e for e in events_data if e['source'] == 'eventbrite']
    # ... then sync filtered events
```

## Disabling Supabase

If you want to use only local database:

1. Remove `SUPABASE_URL` and `SUPABASE_KEY` from `.env`
2. Backend will log "Supabase not configured" and continue with local DB
3. No changes needed - fully backward compatible

---

**Next Steps**: Once Supabase is configured, scraped events will automatically be shared across all users for each city!
