# Deployment Checklist - Live Scraping + Supabase

## Pre-Deployment Review

### Code Changes
- [x] backend/scraper_integration.py - Added Supabase sync
- [x] backend/supabase_integration.py - New Supabase manager
- [x] backend/main.py - Updated scrape response
- [x] .env.example - Added Supabase config
- [x] backend/requirements.txt - Added supabase dependency
- [x] fronto/App.tsx - Already supports live polling (previous iteration)

### Documentation
- [x] SUPABASE_SETUP.md - Complete setup guide
- [x] SUPABASE_QUICK_START.md - 5-minute quick start
- [x] SUPABASE_EVENTS_INTEGRATION.md - Technical details
- [x] ARCHITECTURE.md - System architecture
- [x] LIVE_SCRAPING_FEATURE.md - Feature documentation
- [x] IMPLEMENTATION_SUMMARY.md - Implementation overview

## Pre-Production Setup

### Local Testing (Dev Environment)

#### Step 1: Install Dependencies
```bash
cd /home/workspace/inyAcity/backend
pip install -r requirements.txt
```
- [ ] Verify supabase==2.1.5 installed
- [ ] Verify all other dependencies installed

#### Step 2: Configure Local .env
```bash
# Copy example and fill in
cp .env.example .env

# Edit .env with:
DATABASE_URL=postgresql+asyncpg://nocturne:nocturne@localhost:5432/nocturne
# Leave SUPABASE_URL and SUPABASE_KEY commented for now
```
- [ ] DATABASE_URL points to local PostgreSQL
- [ ] File saved

#### Step 3: Verify Local Database
```bash
# Connect to local PostgreSQL
psql postgresql://nocturne:nocturne@localhost:5432/nocturne

# Check tables exist
\dt
```
- [ ] events table exists
- [ ] subscriptions table exists
- [ ] email_logs table exists

#### Step 4: Start Backend
```bash
cd /home/workspace/inyAcity/backend
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000
```
- [ ] Server starts on port 8000
- [ ] No errors in console
- [ ] GET http://localhost:8000/health returns 200

#### Step 5: Test Local Scraping (Without Supabase)
```bash
# Click city in frontend
# Click "REFRESH EVENTS (1/1)"
```
- [ ] Button changes to "REFRESHING EVENTS..."
- [ ] After 5-30 seconds, events appear
- [ ] Events show "Found X new events"
- [ ] Button becomes "REFRESH USED (0/1)"
- [ ] Check database: `SELECT COUNT(*) FROM events WHERE city_id='ca--los-angeles';`

### Supabase Setup (Staging/Production)

#### Step 1: Create Supabase Project
1. Go to https://supabase.com
2. Sign up or log in
3. Click "New Project"
4. Name: `nocturne-staging` (or `nocturne-prod`)
5. Region: Choose closest to users
6. Database password: Generate & store securely
7. Click "Create new project"
8. Wait 3-5 minutes for project to initialize

- [ ] Project created
- [ ] Status shows "Active"
- [ ] Can access dashboard

#### Step 2: Get Credentials
1. Go to Settings → API
2. Copy:
   - **Project URL** (e.g., `https://xyzabc.supabase.co`)
   - **anon public key** (starts with `eyJhbGc...`)
3. Keep these secret!

- [ ] SUPABASE_URL copied
- [ ] SUPABASE_KEY copied
- [ ] Values stored securely

#### Step 3: Create Database Tables
1. Go to SQL Editor in Supabase
2. Click "New Query"
3. Paste SQL from `SUPABASE_SETUP.md` (the CREATE TABLE section)
4. Click "Run"

SQL to execute:
```sql
CREATE TABLE events (
    id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
    title TEXT NOT NULL,
    link TEXT UNIQUE NOT NULL,
    date DATE NOT NULL,
    time TEXT,
    location TEXT,
    description TEXT,
    source TEXT,
    city_id TEXT NOT NULL,
    synced_at TIMESTAMP DEFAULT now(),
    last_scraped TIMESTAMP DEFAULT now(),
    created_at TIMESTAMP DEFAULT now()
);

CREATE INDEX idx_events_city ON events(city_id);
CREATE INDEX idx_events_city_date ON events(city_id, date);
CREATE INDEX idx_events_last_scraped ON events(last_scraped DESC);

CREATE TABLE subscriptions (
    id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
    email TEXT NOT NULL,
    city_id TEXT NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT now(),
    unsubscribed_at TIMESTAMP,
    UNIQUE(email, city_id)
);

CREATE INDEX idx_subscriptions_email ON subscriptions(email);
CREATE INDEX idx_subscriptions_city ON subscriptions(city_id, is_active);
- [ ] All tables created successfully
- [ ] No errors in SQL editor
- [ ] Can see tables in "Table Editor" tab

#### Step 4: Enable Row Level Security (RLS)

For `events` table:
1. Go to "Authentication" → "Policies"
2. Click "events" table
3. Click "New Policy"
4. Choose "FOR SELECT"
5. Leave "Using" empty (allows all)
6. Click "Save"

For `subscriptions` table:
1. Click "subscriptions" table
2. Click "New Policy"
3. Choose "FOR INSERT" (Build a new policy from scratch)
4. For "Roles", select `anon`.
5. For "WITH CHECK expression", enter `is_active = TRUE`.
6. Click "Save"

- [ ] events table RLS policy created
- [ ] subscriptions table RLS policy created
- [ ] Policies enabled

#### Step 5: Test Supabase Connection
1. Go to Table Editor
2. Click `events` table
3. Should be empty
4. Go to SQL Editor
5. Run: `SELECT COUNT(*) FROM events;`
6. Should return `0`

- [ ] Can access events table
- [ ] Table is empty
- [ ] No errors

### Staging Deployment

#### Step 1: Update Staging .env
```bash
# On staging server
cd /home/workspace/inyAcity

# Create/edit .env with:
DATABASE_URL=postgresql+asyncpg://...staging...
SUPABASE_URL=https://your-staging-project.supabase.co
SUPABASE_KEY=eyJhbGc...staging-key...
```

- [ ] DATABASE_URL is staging database
- [ ] SUPABASE_URL is staging project
- [ ] SUPABASE_KEY is staging key

#### Step 2: Start Staging Backend
```bash
cd backend
python -m uvicorn main:app --host 0.0.0.0 --port 8000
```

- [ ] Server starts
- [ ] No errors
- [ ] Health check: `curl http://staging-ip:8000/health`

#### Step 3: Test Staging Scraping with Supabase
```bash
# In frontend, test scraping
# Click city → Click "REFRESH EVENTS"
```

- [ ] Events scraped successfully
- [ ] Button becomes "REFRESH USED (0/1)"
- [ ] Check Supabase Table Editor → events table
- [ ] See events in Supabase table

#### Step 4: Verify Sync
1. Go to Supabase dashboard
2. Table Editor → events table
3. Should see scraped events
4. Check `synced_at` timestamp
5. Should be recent

- [ ] Events appear in Supabase
- [ ] synced_at timestamps are recent
- [ ] Event count matches frontend

#### Step 5: Test Multi-User Sharing
1. In different browser/window
2. Navigate to same city
3. Should see events that previous user scraped
4. Scrape different events in first browser
5. Refresh second browser
6. Should see all combined events

- [ ] Second user sees first user's scraped events
- [ ] Events are shared correctly
- [ ] No duplicates

### Production Deployment

#### Step 1: Create Production Supabase Project
Follow "Supabase Setup" section above with `nocturne-prod` project name

- [ ] Production Supabase project created
- [ ] Credentials obtained
- [ ] Tables created
- [ ] RLS policies enabled

#### Step 2: Update Production .env
```bash
# On production server
DATABASE_URL=postgresql+asyncpg://...production...
SUPABASE_URL=https://your-prod-project.supabase.co
SUPABASE_KEY=eyJhbGc...prod-key...

# Also set
ALLOWED_ORIGINS=https://yourdomain.com,https://www.yourdomain.com
```

- [ ] DATABASE_URL is production database
- [ ] SUPABASE_URL is production project
- [ ] SUPABASE_KEY is production key
- [ ] ALLOWED_ORIGINS includes your domain

#### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

- [ ] supabase==2.1.5 installed
- [ ] No errors

#### Step 4: Start Production Backend
```bash
# Using gunicorn + uvicorn workers
gunicorn main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --host 0.0.0.0 \
  --port 8000
```

- [ ] Server starts with multiple workers
- [ ] No errors
- [ ] Health check returns 200

#### Step 5: Verify Production
```bash
# Test health endpoint
curl https://yourdomain.com/api/health

# Should return JSON with status and event counts
```

- [ ] Health check succeeds
- [ ] Event counts are accurate
- [ ] No errors in logs

#### Step 6: Monitor Initial Traffic
```bash
# Watch logs for errors
tail -f logs/app.log | grep -i error

# Watch for successful scrapes
tail -f logs/app.log | grep -i scrape

# Watch for Supabase syncs
tail -f logs/app.log | grep -i supabase
```

- [ ] No errors appearing
- [ ] Successful scrapes logged
- [ ] Supabase syncs logged

### Post-Deployment Testing

#### Functional Tests
```bash
# Test 1: Scrape without Supabase (should still work)
# Temporarily comment out SUPABASE_KEY
# Scrape → Events appear
# Check logs for "Supabase not configured"

# Test 2: Scrape with Supabase
# Uncomment SUPABASE_KEY
# Scrape → Events appear
# Check Supabase for synced events

# Test 3: Multi-user sharing
# User A scrapes → Events in Supabase
# User B sees events without scraping

# Test 4: Session limit
# User scrapes (button disabled)
# Navigate to different city (button still disabled)
# Refresh page (button still disabled)

# Test 5: Event counting
# Scrape → Should show "Found X new events"
# Count should match Supabase table
```

- [ ] All functional tests pass

#### Performance Tests
```bash
# Test 1: Scrape latency
# Time how long scrape takes (should be 5-30 seconds)

# Test 2: Polling latency
# Check if events appear quickly (within 1 second)

# Test 3: Supabase sync latency
# Scrape → Check timestamp in Supabase
# Should sync within 1-2 seconds
```

- [ ] Scrape completes in acceptable time
- [ ] Events appear in reasonable time
- [ ] Supabase syncs quickly

#### Error Handling Tests
```bash
# Test 1: Network error during scrape
# Disconnect Supabase credentials
# Scrape → Should fail gracefully
# Events should still save locally

# Test 2: Database error
# Simulate database connection loss
# Scrape → Should handle gracefully
# Check error logs

# Test 3: Supabase timeout
# Reduce Supabase timeout value
# Scrape → Should timeout and continue
```

- [ ] All errors handled gracefully
- [ ] Users see helpful error messages
- [ ] System continues operating

### Production Sign-Off

#### Final Checklist
- [ ] Code deployed to production
- [ ] Environment variables set correctly
- [ ] Supabase project active and configured
- [ ] Database tables created with indexes
- [ ] RLS policies enabled
- [ ] Backend server running with multiple workers
- [ ] Health check endpoint responding
- [ ] Frontend can reach backend API
- [ ] Initial scrape test successful
- [ ] Events synced to Supabase
- [ ] Logs being collected and monitored
- [ ] Error alerts configured

#### Documentation
- [ ] Deployment guide provided to ops team
- [ ] Rollback procedure documented
- [ ] Support guide created for users
- [ ] Monitoring dashboard set up

#### Monitoring Setup
- [ ] Log aggregation tool configured
- [ ] Error tracking enabled
- [ ] Database monitoring active
- [ ] API performance monitored
- [ ] Supabase metrics tracked

### Rollback Plan

If issues occur:

1. **Check Backend Logs**
   ```bash
   tail -f logs/app.log | grep -i error
   ```

2. **Revert Supabase**
   - Remove SUPABASE_KEY from .env
   - Backend continues with local DB only
   - No events synced but scraping works

3. **Revert Code**
   - Restore previous backend code
   - Restart server

4. **Contact Supabase Support**
   - If Supabase having issues
   - Check status page: supabase.com/status

## Maintenance

### Regular Tasks

**Daily**
- [ ] Check error logs for issues
- [ ] Verify Supabase table size is growing

**Weekly**
- [ ] Review Supabase metrics
- [ ] Check scraper output for anomalies
- [ ] Verify sync success rates

**Monthly**
- [ ] Clean up old email logs (optional)
- [ ] Review Supabase billing
- [ ] Test disaster recovery

### Common Maintenance

**Add New City**
1. Add to `SUPPORTED_LOCATIONS` in config.json
2. Restart backend (picks up new config)
3. City appears in frontend immediately

**Update Event Sources**
1. Modify scraper configuration
2. Restart backend/scraper
3. New events synced automatically

**Debug Supabase Issues**
1. Check SQL queries in Supabase dashboard
2. Review RLS policies
3. Check rate limiting (free tier has limits)

---

**Checklist Version**: 1.0  
**Last Updated**: February 5, 2026  
**Status**: Ready for Deployment
